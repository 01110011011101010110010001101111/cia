{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1 (Quantization here, training in quantize_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization code from Song Han's TinyML class\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantized_range(bitwidth):\n",
    "    quantized_max = (1 << (bitwidth - 1)) - 1\n",
    "    quantized_min = -(1 << (bitwidth - 1))\n",
    "    return quantized_min, quantized_max\n",
    "\n",
    "def get_quantization_scale_and_zero_point(fp_tensor, bitwidth):\n",
    "    \"\"\"\n",
    "    get quantization scale for single tensor\n",
    "    :param fp_tensor: [torch.(cuda.)Tensor] floating tensor to be quantized\n",
    "    :param bitwidth: [int] quantization bit width\n",
    "    :return:\n",
    "        [float] scale\n",
    "        [int] zero_point\n",
    "    \"\"\"\n",
    "    quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
    "    fp_max = fp_tensor.max()\n",
    "    fp_min = fp_tensor.min()\n",
    "\n",
    "    # scale\n",
    "    scale = (fp_max - fp_min) / (quantized_max - quantized_min)\n",
    "    # zero_point\n",
    "    zero_point = ((quantized_min - fp_min / scale))\n",
    "\n",
    "    # clip the zero_point to fall in [quantized_min, quantized_max]\n",
    "    if zero_point < quantized_min:\n",
    "        zero_point = quantized_min\n",
    "    elif zero_point > quantized_max:\n",
    "        zero_point = quantized_max\n",
    "    else: # convert from float to int using round()\n",
    "        zero_point = round(zero_point)\n",
    "    return scale, int(zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_quantize(fp_tensor, bitwidth, scale, zero_point, dtype=np.int8) -> np.array:\n",
    "    \"\"\"\n",
    "    linear quantization for single fp_tensor\n",
    "      from\n",
    "        r = fp_tensor = (quantized_tensor - zero_point) * scale\n",
    "      we have,\n",
    "        q = quantized_tensor = int(round(fp_tensor / scale)) + zero_point\n",
    "    :param tensor: [np.array] floating tensor to be quantized\n",
    "    :param bitwidth: [int] quantization bit width\n",
    "    :param scale: [float] scaling factor\n",
    "    :param zero_point: [int] the desired centroid of tensor values\n",
    "    :return:\n",
    "        [np.array] quantized tensor whose values are integers\n",
    "    \"\"\"\n",
    "    # assert(fp_tensor is np.array)\n",
    "    assert(isinstance(scale, float))\n",
    "    assert(isinstance(zero_point, int))\n",
    "\n",
    "    # scale the fp_tensor\n",
    "    scaled_tensor = fp_tensor/scale\n",
    "    # round the floating value to integer value\n",
    "    rounded_tensor = (np.round(scaled_tensor)) #.to(torch.int8)\n",
    "\n",
    "    # print(rounded_tensor.dtype)\n",
    "\n",
    "    # shift the rounded_tensor to make zero_point 0\n",
    "    shifted_tensor = rounded_tensor + zero_point\n",
    "\n",
    "    # clamp the shifted_tensor to lie in bitwidth-bit range\n",
    "    quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
    "    quantized_tensor = shifted_tensor.clip(quantized_min, quantized_max)\n",
    "    quantized_tensor = quantized_tensor.astype(np.int8)\n",
    "    return quantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_quantize_feature(fp_tensor, bitwidth):\n",
    "    \"\"\"\n",
    "    linear quantization for feature tensor\n",
    "    :param fp_tensor: [torch.(cuda.)Tensor] floating feature to be quantized\n",
    "    :param bitwidth: [int] quantization bit width\n",
    "    :return:\n",
    "        [torch.(cuda.)Tensor] quantized tensor\n",
    "        [float] scale tensor\n",
    "        [int] zero point\n",
    "    \"\"\"\n",
    "    scale, zero_point = get_quantization_scale_and_zero_point(fp_tensor, bitwidth)\n",
    "    quantized_tensor = linear_quantize(fp_tensor, bitwidth, scale, zero_point)\n",
    "    return quantized_tensor, scale, zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "fc1.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j2/l2thzykn6k9489f9g899_cvc0000gn/T/ipykernel_77640/2121909562.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"cpu/NN/mnist_100_10.pt\")\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"cpu/NN/mnist_100_10.pt\")\n",
    "# checkpoint = torch.load(\"./NN/MNIST_12_layers.pt\")\n",
    "\n",
    "weights_biases = {}\n",
    "\n",
    "for name, param in checkpoint.items():\n",
    "    weights_biases[name] = param.cpu().numpy()  # Convert to numpy array and store\n",
    "\n",
    "int8_quant = {}\n",
    "\n",
    "for key in weights_biases:\n",
    "    print(key)\n",
    "    int8_quant[key] = linear_quantize_feature(weights_biases[key], 3)[0]\n",
    "    # print(int8_quant[key].shape)\n",
    "    # print(weights_biases[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nn(input):\n",
    "    layer1 = np.dot(weights_biases[\"fc1.weight\"], input) + weights_biases[\"fc1.bias\"]\n",
    "    # layer2 = np.dot(weights_biases[\"fc2.weight\"], layer1) + weights_biases[\"fc2.bias\"]\n",
    "    # layer3 = np.dot(weights_biases[\"fc3.weight\"], layer2) + weights_biases[\"fc3.bias\"]\n",
    "    return np.argmax(layer1)\n",
    "\n",
    "def run_quantized_nn(input):\n",
    "    layer1 = np.dot(int8_quant[\"fc1.weight\"], input) + int8_quant[\"fc1.bias\"]\n",
    "    # layer2 = np.dot(int8_quant[\"fc2.weight\"], layer1) + int8_quant[\"fc2.bias\"]\n",
    "    # layer3 = np.dot(int8_quant[\"fc3.weight\"], layer2) + int8_quant[\"fc3.bias\"]\n",
    "    return np.argmax(layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_img_to_numpy_arr(fp):\n",
    "  img = Image.open(fp)\n",
    "\n",
    "  # Convert the image to grayscale\n",
    "  img = img.convert(\"L\")\n",
    "\n",
    "  pixel_values = np.array(img)\n",
    "\n",
    "  img_flattened = pixel_values.reshape(-1)\n",
    "\n",
    "  img_flattened = [0 if x > 127 else 1 for x in img_flattened]\n",
    "\n",
    "  return np.array(img_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(img_flat):\n",
    "    for i in range(1):\n",
    "        flattened_tensor = img_flat  # Replace with your actual data\n",
    "\n",
    "        # Reshape to 28 x 28\n",
    "        image_tensor = flattened_tensor.reshape(10, 10)  # or .reshape(28, 28)\n",
    "\n",
    "        # Plot the image\n",
    "        plt.imshow(image_tensor, cmap=\"gray\")\n",
    "        plt.colorbar()  # Optional: to show the color scale\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ruth/6.2050/fpga-project/images/7.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./images/7.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m flat_image \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_img_to_numpy_arr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m display_image(flat_image)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(run_nn(flat_image))\n",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m, in \u001b[0;36mconvert_img_to_numpy_arr\u001b[0;34m(fp)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_img_to_numpy_arr\u001b[39m(fp):\n\u001b[0;32m----> 2\u001b[0m   img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;66;03m# Convert the image to grayscale\u001b[39;00m\n\u001b[1;32m      5\u001b[0m   img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/6205_python/lib/python3.12/site-packages/PIL/Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ruth/6.2050/fpga-project/images/7.png'"
     ]
    }
   ],
   "source": [
    "fp = \"./images/7.png\"\n",
    "\n",
    "flat_image = convert_img_to_numpy_arr(fp)\n",
    "display_image(flat_image)\n",
    "print(run_nn(flat_image))\n",
    "run_quantized_nn(flat_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.quantization\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': 64}\n",
    "test_kwargs = {'batch_size': 64}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "\n",
    "transform=transforms.Compose([\n",
    "        transforms.Resize((10, 10)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Lambda(lambda x: torch.where(x > 0.7, torch.tensor(1.0), torch.tensor(0.0))),\n",
    "        transforms.Lambda(lambda x: x.view(-1))\n",
    "        ])\n",
    "\n",
    "def filter_0_and_1(dataset):\n",
    "    indices = [i for i, target in enumerate(dataset.targets) if target == 0 or target == 1]\n",
    "    dataset.targets = dataset.targets[indices]\n",
    "    dataset.data = dataset.data[indices]\n",
    "    return dataset\n",
    "\n",
    "test_dataset = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transform)\n",
    "# test_dataset = filter_0_and_1(test_dataset)\n",
    "\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                       transform=transform)\n",
    "# dataset2 = filter_0_and_1(dataset2)\n",
    "\n",
    "# Create the DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(test_dataset)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7428888888888889\n"
     ]
    }
   ],
   "source": [
    "total = 9000\n",
    "correct = 0\n",
    "\n",
    "for i in range(total):\n",
    "    img = dataset2[i][0]\n",
    "    #display_image(img)\n",
    "    label = dataset2[i][1]\n",
    "    # print(run_nn(img))\n",
    "    # print(run_quantized_nn(img))\n",
    "    if run_quantized_nn(img) == label:\n",
    "        correct += 1\n",
    "\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized NN answer:  9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = test_dataset[i][0]\n",
    "display_image(img)\n",
    "print(\"Quantized NN answer: \", run_quantized_nn(img))\n",
    "\n",
    "int8_quant[\"fc1.weight\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save it to .mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output image saved at nn.mem\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "nn_output = 10\n",
    "\n",
    "def make_num(list, bits):\n",
    "    number = 0\n",
    "    #for i, val in enumerate(reversed(list)):\n",
    "        #number += (val << i*bits)\n",
    "    for i, val in enumerate(list):\n",
    "        number += (val << i*bits)\n",
    "    return number\n",
    "\n",
    "with open(f'nn.mem', 'w') as f:\n",
    "        for y in range(N):\n",
    "            for x in range(nn_output):\n",
    "                f.write(f'{make_num([int8_quant[\"fc1.weight\"][x][y]], 3)}\\n')\n",
    "\n",
    "print('Output image saved at nn.mem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6205_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
